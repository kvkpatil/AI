# Importing the necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline


# Importing the csv file
data = pd.read_csv('Hr.csv')


data.shape


data.columns


data.head()


#Looking for missing data
data.info()


Data Visualisation

# A new pandas Dataframe is created to analyze department wise performance as asked.
dept = data.iloc[:,[5,27]].copy()
dept_per = dept.copy()



# Finding out the mean performance of all the departments and plotting its bar graph
dept_per.groupby(by='EmpDepartment')['PerformanceRating'].mean()



plt.figure(figsize=(10, 4.5))
sns.barplot(x=dept_per['EmpDepartment'], y=dept_per['PerformanceRating'])
plt.show()



# Analyze each department separately
dept_per.groupby(by='EmpDepartment')['PerformanceRating'].value_counts()



# Creating a new dataframe to analyze each department separately
department = pd.get_dummies(dept_per['EmpDepartment'])
performance = pd.DataFrame(dept_per['PerformanceRating'])
dept_rating = pd.concat([department,performance],axis=1)



plt.figure(figsize=(15, 10))
plt.subplot(2, 3, 1)
sns.barplot(x=dept_rating['PerformanceRating'], y=dept_rating['Sales'])
plt.subplot(2, 3, 2)
sns.barplot(x=dept_rating['PerformanceRating'], y=dept_rating['Development'])
plt.subplot(2, 3, 3)
sns.barplot(x=dept_rating['PerformanceRating'], y=dept_rating['Research & Development'])
plt.subplot(2, 3, 4)
sns.barplot(x=dept_rating['PerformanceRating'], y=dept_rating['Human Resources'])
plt.subplot(2, 3, 5)
sns.barplot(x=dept_rating['PerformanceRating'], y=dept_rating['Finance'])
plt.subplot(2, 3, 6)
sns.barplot(x=dept_rating['PerformanceRating'], y=dept_rating['Data Science'])
plt.show()



Data Processing

enc = LabelEncoder()
for i in (2, 3, 4, 5, 6, 7, 16, 26):
    data.iloc[:, i] = enc.fit_transform(data.iloc[:, i])
data.head()



# Finding out the correlation coeffecient to find out which predictors are significan
data.corr()


# Dropping the first columns as it is of no use for analysis.
data.drop(['EmpNumber'],inplace=True,axis=1)


data.head()


# Here we have selected only the important columns
y = data.PerformanceRating
#X = data.iloc[:,0:-1] All predictors were selected it resulted in dropping of accura
X = data.iloc[:,[4,5,9,16,20,21,22,23,24]] # Taking only variables with correlation c
X.head()


# Splitting into train and test for calculating the accuracy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)


# Standardization technique is used
sc = StandardScaler()
X_train = sc.fit_transform(X_train)


X_train.shape


X_test.shape


Model: Random Forest with GridSearchCV


from sklearn.ensemble import RandomForestClassifier
classifier_rfg = RandomForestClassifier(random_state=33, n_estimators=23)
parameters = [{'min_samples_split': [2, 3, 4, 5], 'criterion': ['gini', 'entropy'], 'min_samples_split': [2, 3, 4, 5]}]
model_gridrf = GridSearchCV(estimator=classifier_rfg, param_grid=parameters, scoring='accuracy')
model_gridrf.fit(X_train, y_train)



model_gridrf.best_params_
X_test = sc.transform(X_test)


# Predicting the model
y_predict_rf = model_gridrf.predict(X_test)


# Finding accuracy, precision, recall and confusion matrix
print(accuracy_score(y_test,y_predict_rf))
print(classification_report(y_test,y_predict_rf))


confusion_matrix(y_test,y_predict_rf)
